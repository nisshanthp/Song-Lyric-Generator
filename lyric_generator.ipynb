{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics Set:\n",
      "awake\n",
      "said\n",
      "ever\n",
      "cannot\n",
      "ill\n",
      "away\n",
      "wanted\n",
      "plannin\n",
      "really\n",
      "before\n",
      "flag\n",
      "power\n",
      "keepin\n",
      "wake\n",
      "spazzin\n",
      "vipers\n",
      "are\n",
      "gas\n",
      "crash\n",
      "mothers\n",
      "bitter\n",
      "comes\n",
      "pierre\n",
      "tools\n",
      "jesus\n",
      "its\n",
      "ye\n",
      "pie\n",
      "reveal\n",
      "job\n",
      "hell\n",
      "keef\n",
      "thought\n",
      "couldnt\n",
      "feel\n",
      "picture\n",
      "sanctuary\n",
      "dont\n",
      "though\n",
      "may\n",
      "excitebike\n",
      "pour\n",
      "gave\n",
      "no\n",
      "live\n",
      "written\n",
      "dancin\n",
      "judas\n",
      "woah\n",
      "come\n",
      "longer\n",
      "em\n",
      "oohooh\n",
      "stars\n",
      "with\n",
      "conversation\n",
      "kiss\n",
      "doing\n",
      "began\n",
      "deity\n",
      "slave\n",
      "at\n",
      "350s\n",
      "down\n",
      "survived\n",
      "mean\n",
      "life\n",
      "told\n",
      "forget\n",
      "not\n",
      "tryin\n",
      "want\n",
      "okay\n",
      "favor\n",
      "bearing\n",
      "pure\n",
      "good\n",
      "wrestlin\n",
      "perfect\n",
      "them\n",
      "all\n",
      "morning\n",
      "bike\n",
      "heart\n",
      "sun\n",
      "wore\n",
      "water\n",
      "abraham\n",
      "hallelujah\n",
      "statues\n",
      "needin\n",
      "after\n",
      "youth\n",
      "head\n",
      "new\n",
      "become\n",
      "portions\n",
      "juice\n",
      "dude\n",
      "weapons\n",
      "hide\n",
      "use\n",
      "praising\n",
      "follow\n",
      "say\n",
      "wooooh\n",
      "glory\n",
      "breath\n",
      "tyler\n",
      "833\n",
      "millisecond\n",
      "prayed\n",
      "level\n",
      "loves\n",
      "hand\n",
      "me\n",
      "lie\n",
      "woke\n",
      "couldve\n",
      "safe\n",
      "have\n",
      "wealth\n",
      "give\n",
      "side\n",
      "every\n",
      "lay\n",
      "cant\n",
      "adam\n",
      "forbes\n",
      "hair\n",
      "jezebel\n",
      "mike\n",
      "garage\n",
      "raise\n",
      "everything\n",
      "bust\n",
      "shalt\n",
      "be\n",
      "thing\n",
      "being\n",
      "on\n",
      "drivin\n",
      "worship\n",
      "lows\n",
      "composure\n",
      "strong\n",
      "chasin\n",
      "word\n",
      "only\n",
      "i’m\n",
      "john\n",
      "drank\n",
      "dad\n",
      "culture\n",
      "people\n",
      "like\n",
      "high\n",
      "find\n",
      "been\n",
      "of\n",
      "tubin\n",
      "gotta\n",
      "hands\n",
      "lemonade\n",
      "tribe\n",
      "night\n",
      "supernova\n",
      "locked\n",
      "chickfila\n",
      "im\n",
      "since\n",
      "switch\n",
      "ima\n",
      "always\n",
      "wrestle\n",
      "luke\n",
      "upon\n",
      "line\n",
      "music\n",
      "christ\n",
      "but\n",
      "steppin\n",
      "from\n",
      "prayers\n",
      "feeling\n",
      "clean\n",
      "lying\n",
      "yandhi\n",
      "wide\n",
      "so\n",
      "wild\n",
      "soul\n",
      "lookin\n",
      "need\n",
      "typewrite\n",
      "set\n",
      "solar\n",
      "dry\n",
      "evening\n",
      "eve\n",
      "and\n",
      "move\n",
      "falls\n",
      "thirteenth\n",
      "accept\n",
      "heal\n",
      "it\n",
      "him\n",
      "any\n",
      "highs\n",
      "matter\n",
      "up\n",
      "cool\n",
      "attitude\n",
      "try\n",
      "greedy\n",
      "is\n",
      "shall\n",
      "promise\n",
      "pride\n",
      "free\n",
      "now\n",
      "yall\n",
      "ranch\n",
      "lean\n",
      "amendment\n",
      "sing\n",
      "works\n",
      "same\n",
      "whom\n",
      "spoil\n",
      "rock\n",
      "bondage\n",
      "went\n",
      "commander\n",
      "wonderful\n",
      "hold\n",
      "can\n",
      "even\n",
      "please\n",
      "i\n",
      "fight\n",
      "youre\n",
      "block\n",
      "tried\n",
      "ride\n",
      "drive\n",
      "apple\n",
      "king\n",
      "slower\n",
      "artist\n",
      "product\n",
      "as\n",
      "peek\n",
      "scream\n",
      "nothin\n",
      "make\n",
      "bright\n",
      "soldiers\n",
      "devil\n",
      "out\n",
      "who\n",
      "selfies\n",
      "lord\n",
      "number\n",
      "newborn\n",
      "best\n",
      "jive\n",
      "dark\n",
      "then\n",
      "else\n",
      "tryna\n",
      "treaty\n",
      "his\n",
      "woo\n",
      "noah\n",
      "riding\n",
      "theyre\n",
      "somebody\n",
      "how\n",
      "chauffeur\n",
      "through\n",
      "soda\n",
      "tree\n",
      "sigel\n",
      "pray\n",
      "letter\n",
      "cup\n",
      "own\n",
      "saviour\n",
      "fruit\n",
      "saved\n",
      "watch\n",
      "screamin\n",
      "latitude\n",
      "leaves’ll\n",
      "sand\n",
      "bruises\n",
      "judge\n",
      "time\n",
      "we\n",
      "hes\n",
      "05\n",
      "white\n",
      "in\n",
      "hiding\n",
      "should\n",
      "mad\n",
      "grammys\n",
      "chance\n",
      "indoctrinate\n",
      "brothers\n",
      "a\n",
      "christlike\n",
      "faith\n",
      "til\n",
      "ah\n",
      "end\n",
      "what\n",
      "talk\n",
      "spirits\n",
      "divide\n",
      "sunday\n",
      "mighty\n",
      "bow\n",
      "daughter\n",
      "focused\n",
      "thats\n",
      "text\n",
      "movie\n",
      "sure\n",
      "restin\n",
      "likes\n",
      "stab\n",
      "truth\n",
      "green\n",
      "made\n",
      "gates\n",
      "descendants\n",
      "these\n",
      "ours\n",
      "do\n",
      "decimal\n",
      "oh\n",
      "reflect\n",
      "to\n",
      "go\n",
      "still\n",
      "was\n",
      "yo\n",
      "charge\n",
      "836\n",
      "light\n",
      "god\n",
      "our\n",
      "right\n",
      "enjoy\n",
      "stand\n",
      "nod\n",
      "praise\n",
      "brightest\n",
      "fool\n",
      "did\n",
      "let\n",
      "forgive\n",
      "prices\n",
      "tell\n",
      "if\n",
      "or\n",
      "bet\n",
      "sleeve\n",
      "train\n",
      "powers\n",
      "excellent\n",
      "either\n",
      "fifty\n",
      "alive\n",
      "obey\n",
      "thou\n",
      "horses\n",
      "more\n",
      "you\n",
      "daughters\n",
      "nobody\n",
      "heavens\n",
      "listen\n",
      "monday\n",
      "strengthen\n",
      "replied\n",
      "storm\n",
      "your⁠\n",
      "gods\n",
      "forgave\n",
      "laundry\n",
      "another\n",
      "one\n",
      "take\n",
      "rain\n",
      "short\n",
      "there\n",
      "reid\n",
      "aint\n",
      "each\n",
      "next\n",
      "spring\n",
      "supplied\n",
      "wretch\n",
      "week\n",
      "died\n",
      "the\n",
      "livin\n",
      "la\n",
      "had\n",
      "home\n",
      "race\n",
      "army\n",
      "single\n",
      "know\n",
      "never\n",
      "lets\n",
      "bootin\n",
      "way\n",
      "playin\n",
      "land\n",
      "flood\n",
      "why\n",
      "apiece\n",
      "gram\n",
      "ain’t\n",
      "looking\n",
      "some\n",
      "for\n",
      "perry\n",
      "chlorine\n",
      "bein\n",
      "hath\n",
      "gon\n",
      "front\n",
      "wanna\n",
      "yeah\n",
      "ultrabeam\n",
      "done\n",
      "advice\n",
      "different\n",
      "ive\n",
      "put\n",
      "bleached\n",
      "here\n",
      "03\n",
      "three\n",
      "yourself\n",
      "son\n",
      "my\n",
      "starve\n",
      "got\n",
      "keep\n",
      "four\n",
      "referee\n",
      "tellin\n",
      "cover\n",
      "very\n",
      "starts\n",
      "wantin\n",
      "seein\n",
      "everybody\n",
      "closed\n",
      "sons\n",
      "greatest\n",
      "irs\n",
      "turn\n",
      "well\n",
      "onto\n",
      "yard\n",
      "off\n",
      "hour\n",
      "lve\n",
      "grace\n",
      "your\n",
      "buttons\n",
      "much\n",
      "think\n",
      "baby\n",
      "tithe\n",
      "stretch\n",
      "draw\n",
      "by\n",
      "throne\n",
      "close\n",
      "indeed\n",
      "get\n",
      "they\n",
      "because\n",
      "arguing\n",
      "strength\n",
      "look\n",
      "pressin\n",
      "second\n",
      "faithfulness\n",
      "old\n",
      "half\n",
      "plus\n",
      "lifelike\n",
      "start\n",
      "that\n",
      "felt\n",
      "love\n",
      "over\n",
      "inside\n",
      "just\n",
      "mirage\n",
      "searchin\n",
      "chief\n",
      "darkness\n",
      "book\n",
      "push\n",
      "neighbor\n",
      "loose\n",
      "shine\n",
      "help\n",
      "movin\n",
      "minute\n",
      "see\n",
      "radical\n",
      "goin\n",
      "die\n",
      "wah\n",
      "will\n",
      "us\n",
      "twice\n",
      "man\n",
      "minds\n",
      "back\n",
      "anything\n",
      "win\n",
      "temptations\n",
      "cause\n",
      "walk\n",
      "run\n",
      "ahhh\n",
      "family\n",
      "wont\n",
      "nobodys\n",
      "father\n",
      "alone\n",
      "when\n",
      "hard\n",
      "clive\n",
      "too\n",
      "this\n",
      "flow\n",
      "he\n",
      "thy\n",
      "Dataset Size: 286\n",
      "Sample Input Sequence: tensor([226, 124, 474, 124, 474, 315, 409])\n",
      "Sample Target Sequence: tensor([124, 474, 124, 474, 315, 409,  11])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import string\n",
    "\n",
    "class LyricsGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device):\n",
    "        super(LyricsGenerator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = 1  # Set the batch size to 1\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)  # Set batch_first=True\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_seq, hidden):\n",
    "        batch_size = input_seq.size(0)  # Get the batch size\n",
    "        embedded = self.embedding(input_seq)\n",
    "        hidden = self.init_hidden(batch_size)  # Initialize the hidden state with the updated batch size\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(1, batch_size, self.hidden_size).zero_().to(self.device),\n",
    "                  weight.new(1, batch_size, self.hidden_size).zero_().to(self.device))\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics_list):\n",
    "        self.lyrics_list = lyrics_list\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "\n",
    "        self._create_vocab()\n",
    "\n",
    "    def _create_vocab(self):\n",
    "        words = []\n",
    "        for line in self.lyrics_list:\n",
    "            words.extend(line.split())\n",
    "\n",
    "        words = list(set(words))\n",
    "        self.word_to_index = {word: i for i, word in enumerate(words)}\n",
    "        self.index_to_word = {i: word for i, word in enumerate(words)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lyrics_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.lyrics_list[index]\n",
    "        sequence = [self.word_to_index[word] for word in line.split()]\n",
    "        input_seq = torch.tensor(sequence[:-1]).long()  # Convert input sequence to LongTensor\n",
    "        target_seq = torch.tensor(sequence[1:]).long()  # Convert target sequence to LongTensor\n",
    "        return input_seq, target_seq\n",
    "\n",
    "file_path = \"/Users/nisshanth/KanyeCover/Kanye West Lyrics.txt\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Read the first 400 lines of the file contents\n",
    "        file_contents = [line.strip() for line in file.readlines()[:400] if line.strip()]\n",
    "\n",
    "        # Convert the text to lowercase and remove punctuation/special characters/parentheses\n",
    "        file_contents = [\n",
    "            line.lower()\n",
    "            .translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            .replace(\"(\", \"\")\n",
    "            .replace(\")\", \"\")\n",
    "            for line in file_contents\n",
    "            if \"[\" not in line and \"]\" not in line\n",
    "        ]\n",
    "\n",
    "        # Create the lyrics dataset\n",
    "        dataset = LyricsDataset(file_contents)\n",
    "\n",
    "        # Print the lyrics set\n",
    "        print(\"Lyrics Set:\")\n",
    "        for lyric in dataset.word_to_index:\n",
    "            print(lyric)\n",
    "\n",
    "        # Print the dataset size\n",
    "        print(\"Dataset Size:\", len(dataset))\n",
    "\n",
    "        # Print a sample input-target pair\n",
    "        input_seq, target_seq = dataset[0]\n",
    "        print(\"Sample Input Sequence:\", input_seq)\n",
    "        print(\"Sample Target Sequence:\", target_seq)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_size = len(dataset.word_to_index)\n",
    "hidden_size = 128\n",
    "output_size = len(dataset.word_to_index)\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the model instance\n",
    "model = LyricsGenerator(input_size, hidden_size, output_size, device)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the data loader\n",
    "# Create the data loader with the custom collate function\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: (pad_sequence([item[0] for item in batch], batch_first=True), pad_sequence([item[1] for item in batch], batch_first=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/9], Loss: 6.340527057647705\n",
      "Epoch [2/10], Batch [1/9], Loss: 1.798841118812561\n",
      "Epoch [3/10], Batch [1/9], Loss: 1.886411190032959\n",
      "Epoch [4/10], Batch [1/9], Loss: 1.0222667455673218\n",
      "Epoch [5/10], Batch [1/9], Loss: 0.9230043292045593\n",
      "Epoch [6/10], Batch [1/9], Loss: 0.8644943237304688\n",
      "Epoch [7/10], Batch [1/9], Loss: 0.4975454807281494\n",
      "Epoch [8/10], Batch [1/9], Loss: 0.28289297223091125\n",
      "Epoch [9/10], Batch [1/9], Loss: 0.20439575612545013\n",
      "Epoch [10/10], Batch [1/9], Loss: 0.1822577565908432\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (input_seq, target_seq) in enumerate(data_loader):\n",
    "        # Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize hidden state\n",
    "        batch_size = input_seq.size(0)  # Get the batch size\n",
    "        hidden = (torch.zeros(1, batch_size, hidden_size).to(device),  # Initialize the hidden state with zeros\n",
    "                  torch.zeros(1, batch_size, hidden_size).to(device))  # Initialize the cell state with zeros\n",
    "\n",
    "        # Adjust hidden state size if batch size changes\n",
    "        if hidden[0].size(1) != batch_size:\n",
    "            hidden = (hidden[0][:, :batch_size, :], hidden[1][:, :batch_size, :])\n",
    "\n",
    "        # Forward pass\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "        loss = criterion(output.view(-1, output_size), target_seq.view(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Detach hidden state to prevent gradient accumulation\n",
    "        hidden = tuple([h.detach() for h in hidden])\n",
    "\n",
    "        # Print the loss every few batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Lyrics:\n",
      "we have breath works the strength is lying with judas kiss every time conversation will twice that breath the greatest artist restin with us sing til the selfies them indoctrinate hallelujah hallelujah hallelujah hallelujah on la way for a newborn told me in movie truth twice yeah well right fight down for vipers to whom hour til the leaves’ll tribe the week start as water my life gon praising the 350s he he cover ours now you gon what your loves of the powers had chasin felt thats why i was yeah are water down at dancin you on the army heavens up for every minute the strong the irs want gates awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake awake\n"
     ]
    }
   ],
   "source": [
    "# Generate lyrics\n",
    "start_word = \"we\"  # Starting word for generation\n",
    "num_words = 150  # Number of words to generate\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Convert the starting word to index\n",
    "start_index = dataset.word_to_index[start_word]\n",
    "input_word = torch.tensor([[start_index]])  # Add an extra dimension for batch size\n",
    "\n",
    "# Generate the lyrics\n",
    "generated_lyrics = [start_word]\n",
    "hidden = None\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_words):\n",
    "        output, hidden = model(input_word, hidden)\n",
    "        probabilities = nn.functional.softmax(output, dim=2).squeeze(0)\n",
    "        predicted_index = torch.multinomial(probabilities, 1).item()\n",
    "        predicted_word = dataset.index_to_word[predicted_index]\n",
    "\n",
    "        generated_lyrics.append(predicted_word)\n",
    "        input_word = torch.tensor([[predicted_index]])  # Add an extra dimension for batch size\n",
    "\n",
    "# Print the generated lyrics\n",
    "generated_lyrics = \" \".join(generated_lyrics)\n",
    "print(\"Generated Lyrics:\")\n",
    "print(generated_lyrics)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"lyrics_generator.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
